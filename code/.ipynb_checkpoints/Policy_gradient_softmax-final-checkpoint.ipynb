{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy gradient for finite state finite action MDPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSet-up:\\n1) Softmax policy \\n2) Bounded rewards\\n3) Many states and actions\\n4) Classic policy gradient\\n5) Stochastic version\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Set-up:\n",
    "1) Softmax policy \n",
    "2) Bounded rewards\n",
    "3) Many states and actions\n",
    "4) Classic policy gradient\n",
    "5) Stochastic version\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.6f}\".format(x)})\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Random Seed\n",
    "np.random.seed(10) \n",
    "## Problem Setup\n",
    "gamma = 0.9\n",
    "n, m = 10, 5\n",
    "'''\n",
    "Randomly generated probability transition matrix P((s,a) -> s') in R^{|S||A| x |S|}\n",
    "Each row sums up to one\n",
    "'''\n",
    "raw_transition = np.random.uniform(0,1,size=(n*m,n))\n",
    "prob_transition = raw_transition/raw_transition.sum(axis=1,keepdims=1)\n",
    "'''\n",
    "Random positive rewards\n",
    "'''\n",
    "reward = np.random.uniform(0,1,size=(n*m))\n",
    "'''\n",
    "Start state distribution\n",
    "'''\n",
    "rho = np.ones(n)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Input: theta as an array and \n",
    "Ouput: array of probabilites corresponding to each state: [\\pi_{s_1}(.), ...., \\pi_{s_n}(.)]\n",
    "'''\n",
    "def theta_to_policy(theta,n,m):\n",
    "    prob = []\n",
    "    for i in range(n):\n",
    "        norm = np.sum(np.exp(theta[m*i:m*(i+1)]))\n",
    "        for j in range(m*i,m*(i+1)):\n",
    "            prob.append(np.exp(theta[j])/norm)\n",
    "            \n",
    "    return np.asarray(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Get \\Pi_{\\pi}((s) -> (s,a)) in R^{|S| x |S||A|} matrix corresponding to the policy \\pi using the prob vector\n",
    "'''\n",
    "def get_Pi(prob,n,m):\n",
    "    Pi = np.zeros((n,n*m))\n",
    "    for i in range(n):\n",
    "        Pi[i,i*m:(i+1)*m] = prob[i*m:(i+1)*m]\n",
    "    \n",
    "    return Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Input: probability vector, state, action\n",
    "Output: \\nabla_{\\theta} \\pi_{\\theta}(s,a)\n",
    "\n",
    "States go from 0 to n-1 and actons from 0 to m-1\n",
    "'''\n",
    "def grad_state_action(prob,state,action):\n",
    "    grad = np.zeros(n*m)\n",
    "    for j in range(0,m):\n",
    "        if j == action:\n",
    "            grad[m*state + j] = prob[m*state + j]*(1-prob[m*state + j])\n",
    "        else:\n",
    "            grad[m*state + j] = -prob[m*state + action]*prob[m*state + j]\n",
    "            \n",
    "    return grad\n",
    "\n",
    "def grad_state(qvals,prob,state):\n",
    "    grad = np.sum([qvals[state*m + i]*grad_state_action(prob,state,i) for i in range(0,m)],axis=0)\n",
    "    return grad\n",
    "\n",
    "def grad(qvals,prob,d_pi):\n",
    "    grad = np.sum([d_pi[i]*grad_state(qvals,prob,i) for i in range(0,n)],axis=0)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The overall reward function \\ell(\\theta)\n",
    "'''\n",
    "def ell(qvals,prob,rho):\n",
    "    V = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        V[i] = np.sum([qvals[i*m + j]*prob[i*m + j] for j in range(m)])\n",
    "    \n",
    "    ell = np.dot(V,rho)\n",
    "    return ell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration to get the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_vec = np.random.uniform(0,1,size=(n,m))\n",
    "prob_vec = raw_vec/raw_vec.sum(axis=1,keepdims=1)\n",
    "init_policy = prob_vec.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Policy iteration function\n",
    "'''\n",
    "def policy_iter(q_vals,n,m):\n",
    "    new_policy = np.zeros(n*m)\n",
    "    for i in range(n):\n",
    "        idx = np.argmax(q_vals[i*m:(i+1)*m])\n",
    "        new_policy[i*m + idx] = 1\n",
    "    \n",
    "    return new_policy       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting policy [0.251097 0.279849 0.147699 0.056339 0.265016 0.212490 0.257024 0.165946\n",
      " 0.255862 0.108679 0.092219 0.736767 0.003439 0.059852 0.107722 0.520532\n",
      " 0.167367 0.117854 0.141252 0.052995 0.177840 0.323561 0.286905 0.111027\n",
      " 0.100667 0.155611 0.271404 0.198999 0.236796 0.137190 0.188020 0.339627\n",
      " 0.174939 0.259611 0.037802 0.222241 0.105056 0.277653 0.111753 0.283296\n",
      " 0.262628 0.005114 0.162176 0.343151 0.226932 0.206239 0.078903 0.292514\n",
      " 0.182116 0.240228]\n",
      "Final policy [0.000000 0.000000 0.000000 0.000000 1.000000 1.000000 0.000000 0.000000\n",
      " 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 0.000000\n",
      " 0.000000 1.000000 0.000000 0.000000 0.000000 1.000000 0.000000 0.000000\n",
      " 0.000000 0.000000 0.000000 0.000000 1.000000 0.000000 0.000000 1.000000\n",
      " 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000\n",
      " 0.000000 0.000000 0.000000 1.000000 0.000000 0.000000 1.000000 0.000000\n",
      " 0.000000 0.000000]\n"
     ]
    }
   ],
   "source": [
    "curr_policy = np.random.uniform(0,1,size=(n*m))\n",
    "new_policy = init_policy\n",
    "print('Starting policy',init_policy)\n",
    "\n",
    "while np.count_nonzero(curr_policy - new_policy) > 0:\n",
    "    curr_policy = new_policy\n",
    "    Pi = get_Pi(curr_policy,n,m)\n",
    "    mat = np.identity(n*m) - gamma*np.matmul(prob_transition,Pi)\n",
    "    q_vals = np.dot(np.linalg.inv(mat),reward)\n",
    "    new_policy = policy_iter(q_vals,n,m)\n",
    "    \n",
    "print('Final policy',new_policy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.58725179475\n"
     ]
    }
   ],
   "source": [
    "ell_star = ell(q_vals,new_policy,rho)\n",
    "print(ell_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtracking line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ell_theta(theta,rho):\n",
    "    prob = theta_to_policy(theta,n,m)\n",
    "    Pi = get_Pi(prob,n,m)\n",
    "    mat = np.identity(n*m) - gamma*np.matmul(prob_transition,Pi)\n",
    "    qvals = np.dot(np.linalg.inv(mat),reward)\n",
    "    return ell(qvals,prob,rho)\n",
    "    \n",
    "    \n",
    "def find_step(theta,gradient,alpha,beta):\n",
    "    step = alpha\n",
    "    while ell_theta(theta - step*gradient,rho) > ell_theta(theta,rho) - (step/2)*np.linalg.norm(gradient):\n",
    "        step = beta*step\n",
    "    return step\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy gradient in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimality gap 3.55378465153\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Gradient decent\n",
    "'''\n",
    "N = 100000\n",
    "stepsize = 0.01\n",
    "# Parameters for line search\n",
    "alpha = 1\n",
    "beta = 0.7\n",
    "theta = np.random.uniform(0,1,size=n*m)\n",
    "gap = []\n",
    "for k in range(N):\n",
    "    prob = theta_to_policy(theta,n,m)\n",
    "\n",
    "    Pi = get_Pi(prob,n,m)\n",
    "    mat = np.identity(n*m) - gamma*np.matmul(prob_transition,Pi)\n",
    "    qvals = np.dot(np.linalg.inv(mat),reward)\n",
    "\n",
    "    P_theta = np.matmul(Pi,prob_transition)\n",
    "    d_pi = (1-gamma)*np.dot(np.transpose((np.linalg.inv(np.identity(n) - gamma*P_theta))),rho)\n",
    "\n",
    "    gradient = grad(qvals,prob,d_pi)\n",
    "    #     theta += stepsize*gradient\n",
    "\n",
    "    step = find_step(theta,gradient,alpha,beta)\n",
    "    theta += step*gradient\n",
    "    \n",
    "    \n",
    "    if k % 1000 == 0:\n",
    "        avg_reward = ell(qvals,prob,rho)\n",
    "        print('Optimality gap',ell_star - avg_reward)\n",
    "        gap.append(ell_star - avg_reward)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Policy gap',new_policy - theta_to_policy(theta,n,m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reward',reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(np.array(gap))\n",
    "_ = plt.title('Optimality gap during training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
