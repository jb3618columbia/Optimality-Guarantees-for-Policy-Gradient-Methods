{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSet-up:\\n1) Threshold policy \\n2) Bounded rewards\\n3) Many contextx and finite offers \\n4) Classic policy gradient\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Set-up:\n",
    "1) Threshold policy \n",
    "2) Bounded rewards\n",
    "3) Many contextx and finite offers \n",
    "4) Classic policy gradient\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.6f}\".format(x)})\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Emission probability', array([[0.288556, 0.007763, 0.237052, 0.280133, 0.186495],\n",
      "       [0.156018, 0.137463, 0.527838, 0.117370, 0.061311],\n",
      "       [0.230954, 0.321277, 0.001330, 0.172600, 0.273839],\n",
      "       [0.187977, 0.221499, 0.089574, 0.281655, 0.219295],\n",
      "       [0.249558, 0.065395, 0.171728, 0.310086, 0.203233]]))\n",
      "('Latent probability', array([[0.154105, 0.219350, 0.182199, 0.230936, 0.213410],\n",
      "       [0.304408, 0.197204, 0.343507, 0.120684, 0.034197],\n",
      "       [0.156897, 0.059474, 0.432382, 0.024469, 0.326779],\n",
      "       [0.197376, 0.295311, 0.071710, 0.308850, 0.126753],\n",
      "       [0.311186, 0.122042, 0.364499, 0.134228, 0.068046]]))\n",
      "('Offer vector', array([0.392529, 0.093460, 0.821106, 0.151152, 0.384114]))\n",
      "('Reward', array([0.000000, 0.392529, 0.000000, 0.093460, 0.000000, 0.821106,\n",
      "       0.000000, 0.151152, 0.000000, 0.384114, 0.000000, 0.392529,\n",
      "       0.000000, 0.093460, 0.000000, 0.821106, 0.000000, 0.151152,\n",
      "       0.000000, 0.384114, 0.000000, 0.392529, 0.000000, 0.093460,\n",
      "       0.000000, 0.821106, 0.000000, 0.151152, 0.000000, 0.384114,\n",
      "       0.000000, 0.392529, 0.000000, 0.093460, 0.000000, 0.821106,\n",
      "       0.000000, 0.151152, 0.000000, 0.384114, 0.000000, 0.392529,\n",
      "       0.000000, 0.093460, 0.000000, 0.821106, 0.000000, 0.151152,\n",
      "       0.000000, 0.384114]))\n",
      "[[0.057711 0.001553 0.047410 0.056027 0.037299]\n",
      " [0.031204 0.027493 0.105568 0.023474 0.012262]\n",
      " [0.046191 0.064255 0.000266 0.034520 0.054768]\n",
      " [0.037595 0.044300 0.017915 0.056331 0.043859]\n",
      " [0.049912 0.013079 0.034346 0.062017 0.040647]]\n",
      "[0.057711 0.001553 0.047410 0.056027 0.037299 0.031204 0.027493 0.105568\n",
      " 0.023474 0.012262 0.046191 0.064255 0.000266 0.034520 0.054768 0.037595\n",
      " 0.044300 0.017915 0.056331 0.043859 0.049912 0.013079 0.034346 0.062017\n",
      " 0.040647]\n"
     ]
    }
   ],
   "source": [
    "## Random Seed\n",
    "np.random.seed(10) \n",
    "## Problem Setup\n",
    "gamma = 0.9\n",
    "n, m = 5, 5\n",
    "'''\n",
    "Q matrix: Emission probabilities Q(y|x) in R^{|X| x |Y|}\n",
    "'''\n",
    "raw_em = np.random.uniform(0,1,size=(n,m))\n",
    "prob_em = raw_em/raw_em.sum(axis=1,keepdims=1)\n",
    "print('Emission probability',prob_em)\n",
    "'''\n",
    "Latent transitions in R^{|X| x |X|}\n",
    "'''\n",
    "raw_latent = np.random.uniform(0,1,size=(n,n))\n",
    "prob_latent = raw_latent/raw_latent.sum(axis=1,keepdims=1)\n",
    "print('Latent probability',prob_latent)\n",
    "\n",
    "'''\n",
    "Random positive offers in R^{|Y|}\n",
    "'''\n",
    "offer = np.random.uniform(0,1,size=(m))\n",
    "print('Offer vector', offer)\n",
    "'''\n",
    "Reward\n",
    "'''\n",
    "reward = np.zeros(2*n*m)\n",
    "for j in range(m):\n",
    "    for i in range(n):\n",
    "        reward[2*(i*m + j)+1] = offer[j]\n",
    "print('Reward',reward)\n",
    "'''\n",
    "Start state distribution\n",
    "'''\n",
    "rho = prob_em/n\n",
    "print(rho)\n",
    "print(rho.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# Randomly generated probability transition matrix P((s,a) -> s') in R^{|A|(|X||Y|+1) x (|X||Y|+1)} with an absorbing state\n",
    "# Each row sums up to one\n",
    "# Note, the transitons (x,y) -> (x',y') do not depend on y, so there is a block structure\n",
    "# '''\n",
    "# def get_prob_trans(prob_em,prob_latent,n,m):\n",
    "#     prob_trans = np.zeros((2*(n*m + 1),n*m + 1))\n",
    "    \n",
    "#     ## This takes care of action 1 by transitioning only to the absorbing state\n",
    "#     for I in range(n*m):\n",
    "#         prob_trans[2*I+1,-1] = 1\n",
    "    \n",
    "#     ## This is for action 0 which transitions between states; no transition to the absorbing state\n",
    "#     for i in range(n):\n",
    "#         prob_0 = np.zeros(n*m + 1)\n",
    "#         for j in range(n):\n",
    "#             for k in range(m):\n",
    "#                 prob_0[j*m + k] = prob_latent[i,j]*prob_em[j,k]\n",
    "#         for J in range(i*m,(i+1)*m):\n",
    "#             prob_trans[2*J,:] = prob_0\n",
    "    \n",
    "#     ## Accomadoate transitions from the absorbing state\n",
    "#     prob_trans[-2,-1] = 1\n",
    "#     prob_trans[-1,-1] = 1\n",
    "    \n",
    "#     return prob_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prob_trans = get_prob_trans(prob_em,prob_latent,n,m)\n",
    "# print(prob_trans)\n",
    "# print(np.sum(prob_trans[0,:])) ## As a check rows sum upto 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Randomly generated probability transition matrix P((s,a) -> s') in R^{|X||Y|*|A| x |X||Y|} with an absorbing state\n",
    "Each row sums up to one\n",
    "Note, the transitons (x,y) -> (x',y') do not depend on y, so there is a block structure\n",
    "'''\n",
    "def get_prob_trans(prob_em,prob_latent,n,m):\n",
    "    prob_trans = np.zeros((2*n*m,n*m))\n",
    "    \n",
    "    ## This is for action 0 which transitions between states; no transition to the absorbing state\n",
    "    for i in range(n):\n",
    "        prob_0 = np.zeros(n*m)\n",
    "        for j in range(n):\n",
    "            for k in range(m):\n",
    "                prob_0[j*m + k] = prob_latent[i,j]*prob_em[j,k]\n",
    "                \n",
    "        for J in range(i*m,(i+1)*m):\n",
    "            prob_trans[2*J,:] = prob_0\n",
    "    \n",
    "    return prob_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.044468 0.001196 0.036531 ... 0.036649 0.066175 0.043372]\n",
      " [0.000000 0.000000 0.000000 ... 0.000000 0.000000 0.000000]\n",
      " [0.044468 0.001196 0.036531 ... 0.036649 0.066175 0.043372]\n",
      " ...\n",
      " [0.000000 0.000000 0.000000 ... 0.000000 0.000000 0.000000]\n",
      " [0.089795 0.002416 0.073767 ... 0.011685 0.021100 0.013829]\n",
      " [0.000000 0.000000 0.000000 ... 0.000000 0.000000 0.000000]]\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "prob_trans = get_prob_trans(prob_em,prob_latent,n,m)\n",
    "print(prob_trans)\n",
    "print(np.sum(prob_trans[0,:])) ## As a check rows sum upto 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.944261 0.987625 0.456305 0.826123 0.251374 0.597372 0.902832 0.534558\n",
      " 0.590201 0.039282]\n"
     ]
    }
   ],
   "source": [
    "theta = np.random.uniform(0,1,size=2*n)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Input: theta as an array\n",
    "Ouput: array of probabilites corresponding to each (state,action): [\\pi_{(x,y),a}] in R^{|X||Y|*|A|}\n",
    "'''\n",
    "def theta_to_policy(theta,n,m):\n",
    "    prob = np.zeros(2*n*m)\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            prob_acc = 1/(1 + np.exp(-theta[2*i + 1]*(offer[j] - theta[2*i])))\n",
    "            prob[2*(i*m + j) + 1] = prob_acc  ## Action = 1\n",
    "            prob[2*(i*m + j)] = 1 - prob_acc  ## Action = 0\n",
    "            \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.632952 0.367048 0.698523 0.301477 0.530370 0.469630 0.686390 0.313610\n",
      " 0.634881 0.365119 0.513169 0.486831 0.574382 0.425618 0.425222 0.574778\n",
      " 0.562692 0.437308 0.514905 0.485095 0.478932 0.521068 0.523566 0.476434\n",
      " 0.415727 0.584273 0.514963 0.485037 0.480187 0.519813 0.567777 0.432223\n",
      " 0.606508 0.393492 0.510920 0.489080 0.599124 0.400876 0.568880 0.431120\n",
      " 0.501941 0.498059 0.504878 0.495122 0.497732 0.502268 0.504312 0.495688\n",
      " 0.502024 0.497976]\n"
     ]
    }
   ],
   "source": [
    "prob = theta_to_policy(theta,n,m)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Get \\Pi_{\\pi}((s) -> (s,a)) in R^{|X||Y| x |X||Y|*|A|} matrix corresponding to the policy \\pi using the prob vector\n",
    "'''\n",
    "def get_Pi(prob,n,m):\n",
    "    Pi = np.zeros((n*m,2*n*m))\n",
    "    for i in range(n*m):\n",
    "        Pi[i,2*i:2*(i+1)] = prob[2*i:2*(i+1)]\n",
    "    \n",
    "    return Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.632952 0.367048 0.000000 ... 0.000000 0.000000 0.000000]\n",
      " [0.000000 0.000000 0.698523 ... 0.000000 0.000000 0.000000]\n",
      " [0.000000 0.000000 0.000000 ... 0.000000 0.000000 0.000000]\n",
      " ...\n",
      " [0.000000 0.000000 0.000000 ... 0.000000 0.000000 0.000000]\n",
      " [0.000000 0.000000 0.000000 ... 0.495688 0.000000 0.000000]\n",
      " [0.000000 0.000000 0.000000 ... 0.000000 0.502024 0.497976]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Check by comparing \\Pi to the prob vector\n",
    "'''\n",
    "Pi = get_Pi(prob,n,m)\n",
    "print(Pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Q values', array([0.317742, 0.392529, 0.317742, 0.093460, 0.317742, 0.821106,\n",
      "       0.317742, 0.151152, 0.317742, 0.384114, 0.312370, 0.392529,\n",
      "       0.312370, 0.093460, 0.312370, 0.821106, 0.312370, 0.151152,\n",
      "       0.312370, 0.384114, 0.289641, 0.392529, 0.289641, 0.093460,\n",
      "       0.289641, 0.821106, 0.289641, 0.151152, 0.289641, 0.384114,\n",
      "       0.332254, 0.392529, 0.332254, 0.093460, 0.332254, 0.821106,\n",
      "       0.332254, 0.151152, 0.332254, 0.384114, 0.303309, 0.392529,\n",
      "       0.303309, 0.093460, 0.303309, 0.821106, 0.303309, 0.151152,\n",
      "       0.303309, 0.384114]))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Generate qvalues by solving a system of equations\n",
    "'''\n",
    "mat = np.identity(2*n*m) - gamma*np.matmul(prob_trans,Pi)\n",
    "# print(mat)\n",
    "# print(np.linalg.inv(mat))\n",
    "qvals = np.dot(np.linalg.inv(mat),reward)\n",
    "print('Q values',qvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Discounted state-occupancy measure\n",
    "'''\n",
    "P_theta = np.matmul(Pi,prob_trans)\n",
    "d_pi = (1-gamma)*np.dot(np.transpose((np.linalg.inv(np.identity(n*m) - gamma*P_theta))),rho.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.028146 0.000757 0.023122 0.027324 0.018191 0.021661 0.019085 0.073284\n",
      "  0.016295 0.008512 0.026634 0.037051 0.000153 0.019905 0.031580 0.027477\n",
      "  0.032377 0.013093 0.041170 0.032055 0.033710 0.008833 0.023197 0.041886\n",
      "  0.027452]\n",
      " [0.031062 0.000836 0.025518 0.030155 0.020075 0.023905 0.021062 0.080876\n",
      "  0.017983 0.009394 0.029394 0.040889 0.000169 0.021967 0.034852 0.030323\n",
      "  0.035731 0.014449 0.045435 0.035375 0.037202 0.009749 0.025600 0.046225\n",
      "  0.030296]\n",
      " [0.023584 0.000635 0.019375 0.022896 0.015243 0.018151 0.015992 0.061407\n",
      "  0.013654 0.007133 0.022318 0.031046 0.000129 0.016679 0.026462 0.023024\n",
      "  0.027129 0.010971 0.034497 0.026860 0.028247 0.007402 0.019437 0.035098\n",
      "  0.023003]\n",
      " [0.030522 0.000821 0.025074 0.029631 0.019727 0.023490 0.020696 0.079471\n",
      "  0.017671 0.009231 0.028883 0.040179 0.000166 0.021585 0.034246 0.029797\n",
      "  0.035110 0.014198 0.044646 0.034761 0.036556 0.009579 0.025155 0.045422\n",
      "  0.029770]\n",
      " [0.028232 0.000760 0.023193 0.027408 0.018246 0.021727 0.019143 0.073507\n",
      "  0.016345 0.008538 0.026716 0.037164 0.000154 0.019965 0.031676 0.027561\n",
      "  0.032475 0.013133 0.041295 0.032152 0.033813 0.008860 0.023267 0.042014\n",
      "  0.027536]\n",
      " [0.045076 0.001213 0.037030 0.043760 0.029133 0.015789 0.013911 0.053417\n",
      "  0.011878 0.006205 0.040712 0.056634 0.000235 0.030425 0.048271 0.011642\n",
      "  0.013718 0.005547 0.017443 0.013581 0.004379 0.001148 0.003014 0.005442\n",
      "  0.003567]\n",
      " [0.050453 0.001357 0.041448 0.048980 0.032608 0.017672 0.015571 0.059789\n",
      "  0.013295 0.006945 0.045568 0.063389 0.000263 0.034055 0.054030 0.013030\n",
      "  0.015354 0.006209 0.019524 0.015201 0.004902 0.001285 0.003373 0.006091\n",
      "  0.003992]\n",
      " [0.037351 0.001005 0.030684 0.036261 0.024140 0.013083 0.011527 0.044262\n",
      "  0.009842 0.005141 0.033735 0.046928 0.000194 0.025211 0.039999 0.009647\n",
      "  0.011367 0.004597 0.014454 0.011254 0.003629 0.000951 0.002497 0.004509\n",
      "  0.002955]\n",
      " [0.049426 0.001330 0.040604 0.047983 0.031944 0.017313 0.015254 0.058572\n",
      "  0.013024 0.006803 0.044641 0.062099 0.000257 0.033362 0.052930 0.012765\n",
      "  0.015042 0.006083 0.019127 0.014892 0.004802 0.001258 0.003304 0.005967\n",
      "  0.003911]\n",
      " [0.045229 0.001217 0.037156 0.043908 0.029231 0.015842 0.013958 0.053597\n",
      "  0.011918 0.006226 0.040850 0.056825 0.000235 0.030528 0.048435 0.011681\n",
      "  0.013764 0.005566 0.017502 0.013627 0.004394 0.001151 0.003024 0.005460\n",
      "  0.003579]\n",
      " [0.021683 0.000583 0.017813 0.021050 0.014014 0.004444 0.003915 0.015035\n",
      "  0.003343 0.001746 0.047826 0.066530 0.000276 0.035742 0.056707 0.002203\n",
      "  0.002596 0.001050 0.003301 0.002570 0.039057 0.010235 0.026876 0.048530\n",
      "  0.031807]\n",
      " [0.023704 0.000638 0.019473 0.023012 0.015320 0.004858 0.004280 0.016436\n",
      "  0.003655 0.001909 0.052283 0.072731 0.000301 0.039073 0.061992 0.002408\n",
      "  0.002838 0.001148 0.003608 0.002809 0.042697 0.011188 0.029381 0.053053\n",
      "  0.034771]\n",
      " [0.018821 0.000506 0.015462 0.018272 0.012164 0.003858 0.003399 0.013051\n",
      "  0.002902 0.001516 0.041515 0.057750 0.000239 0.031025 0.049223 0.001912\n",
      "  0.002253 0.000911 0.002865 0.002231 0.033903 0.008884 0.023329 0.042125\n",
      "  0.027609]\n",
      " [0.023314 0.000627 0.019153 0.022634 0.015068 0.004778 0.004210 0.016166\n",
      "  0.003595 0.001878 0.051424 0.071536 0.000296 0.038431 0.060973 0.002369\n",
      "  0.002791 0.001129 0.003549 0.002763 0.041995 0.011005 0.028898 0.052181\n",
      "  0.034200]\n",
      " [0.021740 0.000585 0.017859 0.021105 0.014050 0.004456 0.003926 0.015074\n",
      "  0.003352 0.001751 0.047952 0.066705 0.000276 0.035836 0.056856 0.002209\n",
      "  0.002603 0.001052 0.003309 0.002577 0.039159 0.010261 0.026947 0.048657\n",
      "  0.031890]\n",
      " [0.032337 0.000870 0.026565 0.031393 0.020900 0.026160 0.023049 0.088503\n",
      "  0.019679 0.010280 0.009403 0.013081 0.000054 0.007027 0.011149 0.032963\n",
      "  0.038842 0.015707 0.049390 0.038455 0.017960 0.004706 0.012359 0.022316\n",
      "  0.014626]\n",
      " [0.034543 0.000929 0.028378 0.033535 0.022325 0.027944 0.024621 0.094540\n",
      "  0.021022 0.010981 0.010045 0.013973 0.000058 0.007507 0.011910 0.035212\n",
      "  0.041491 0.016779 0.052760 0.041078 0.019185 0.005027 0.013202 0.023838\n",
      "  0.015624]\n",
      " [0.029099 0.000783 0.023905 0.028250 0.018807 0.023540 0.020740 0.079640\n",
      "  0.017709 0.009251 0.008462 0.011771 0.000049 0.006324 0.010033 0.029662\n",
      "  0.034952 0.014135 0.044444 0.034604 0.016161 0.004235 0.011121 0.020081\n",
      "  0.013161]\n",
      " [0.034123 0.000918 0.028032 0.033127 0.022054 0.027604 0.024321 0.093389\n",
      "  0.020766 0.010848 0.009923 0.013803 0.000057 0.007415 0.011765 0.034783\n",
      "  0.040986 0.016575 0.052117 0.040578 0.018952 0.004966 0.013041 0.023548\n",
      "  0.015434]\n",
      " [0.032400 0.000872 0.026617 0.031454 0.020940 0.026210 0.023093 0.088675\n",
      "  0.019718 0.010300 0.009422 0.013106 0.000054 0.007041 0.011171 0.033027\n",
      "  0.038917 0.015738 0.049486 0.038530 0.017995 0.004715 0.012383 0.022359\n",
      "  0.014655]\n",
      " [0.045072 0.001213 0.037027 0.043756 0.029130 0.009557 0.008421 0.032334\n",
      "  0.007190 0.003756 0.042255 0.058780 0.000243 0.031578 0.050101 0.012665\n",
      "  0.014923 0.006035 0.018976 0.014775 0.008524 0.002234 0.005865 0.010591\n",
      "  0.006941]\n",
      " [0.045335 0.001220 0.037243 0.044012 0.029300 0.009613 0.008470 0.032524\n",
      "  0.007232 0.003778 0.042502 0.059124 0.000245 0.031763 0.050394 0.012739\n",
      "  0.015011 0.006070 0.019087 0.014861 0.008574 0.002247 0.005900 0.010653\n",
      "  0.006982]\n",
      " [0.044694 0.001202 0.036716 0.043389 0.028886 0.009477 0.008350 0.032063\n",
      "  0.007130 0.003724 0.041900 0.058287 0.000241 0.031314 0.049681 0.012559\n",
      "  0.014798 0.005984 0.018817 0.014651 0.008452 0.002215 0.005816 0.010502\n",
      "  0.006883]\n",
      " [0.045284 0.001218 0.037202 0.043962 0.029267 0.009602 0.008461 0.032487\n",
      "  0.007224 0.003774 0.042454 0.059057 0.000245 0.031727 0.050337 0.012725\n",
      "  0.014994 0.006063 0.019066 0.014845 0.008564 0.002244 0.005893 0.010641\n",
      "  0.006974]\n",
      " [0.045079 0.001213 0.037033 0.043763 0.029135 0.009559 0.008422 0.032340\n",
      "  0.007191 0.003756 0.042262 0.058789 0.000243 0.031584 0.050109 0.012667\n",
      "  0.014926 0.006036 0.018979 0.014777 0.008525 0.002234 0.005866 0.010593\n",
      "  0.006943]]\n",
      "[[0.105883 0.000158 0.004833 0.005711 0.003802 0.003448 0.003038 0.011664\n",
      "  0.002594 0.001355 0.005730 0.007970 0.000033 0.004282 0.006794 0.004143\n",
      "  0.004881 0.001974 0.006207 0.004833 0.005158 0.001352 0.003550 0.006410\n",
      "  0.004201]\n",
      " [0.006492 0.100175 0.005333 0.006303 0.004196 0.003805 0.003352 0.012873\n",
      "  0.002862 0.001495 0.006323 0.008796 0.000036 0.004726 0.007497 0.004572\n",
      "  0.005387 0.002179 0.006850 0.005334 0.005693 0.001492 0.003917 0.007074\n",
      "  0.004636]\n",
      " [0.004929 0.000133 0.104049 0.004785 0.003186 0.002889 0.002545 0.009774\n",
      "  0.002173 0.001135 0.004801 0.006679 0.000028 0.003588 0.005693 0.003471\n",
      "  0.004090 0.001654 0.005201 0.004050 0.004322 0.001133 0.002974 0.005371\n",
      "  0.003520]\n",
      " [0.006379 0.000172 0.005241 0.106193 0.004123 0.003739 0.003294 0.012649\n",
      "  0.002813 0.001469 0.006213 0.008643 0.000036 0.004643 0.007367 0.004492\n",
      "  0.005294 0.002141 0.006731 0.005241 0.005594 0.001466 0.003849 0.006951\n",
      "  0.004556]\n",
      " [0.005901 0.000159 0.004847 0.005728 0.103814 0.003458 0.003047 0.011700\n",
      "  0.002602 0.001359 0.005747 0.007995 0.000033 0.004295 0.006814 0.004155\n",
      "  0.004896 0.001980 0.006226 0.004848 0.005174 0.001356 0.003561 0.006429\n",
      "  0.004214]\n",
      " [0.006597 0.000177 0.005419 0.006404 0.004263 0.102579 0.002273 0.008726\n",
      "  0.001940 0.001014 0.006521 0.009072 0.000038 0.004874 0.007732 0.002311\n",
      "  0.002723 0.001101 0.003463 0.002696 0.002454 0.000643 0.001689 0.003049\n",
      "  0.001998]\n",
      " [0.007384 0.000199 0.006066 0.007168 0.004772 0.002887 0.102544 0.009767\n",
      "  0.002172 0.001134 0.007299 0.010154 0.000042 0.005455 0.008655 0.002587\n",
      "  0.003048 0.001233 0.003876 0.003018 0.002747 0.000720 0.001890 0.003413\n",
      "  0.002237]\n",
      " [0.005466 0.000147 0.004491 0.005307 0.003533 0.002137 0.001883 0.107231\n",
      "  0.001608 0.000840 0.005404 0.007517 0.000031 0.004038 0.006407 0.001915\n",
      "  0.002257 0.000913 0.002870 0.002234 0.002033 0.000533 0.001399 0.002526\n",
      "  0.001656]\n",
      " [0.007233 0.000195 0.005942 0.007022 0.004675 0.002828 0.002492 0.009568\n",
      "  0.102128 0.001111 0.007151 0.009947 0.000041 0.005344 0.008478 0.002534\n",
      "  0.002986 0.001208 0.003797 0.002957 0.002691 0.000705 0.001851 0.003343\n",
      "  0.002191]\n",
      " [0.006619 0.000178 0.005438 0.006426 0.004278 0.002588 0.002280 0.008756\n",
      "  0.001947 0.101017 0.006543 0.009102 0.000038 0.004890 0.007758 0.002319\n",
      "  0.002733 0.001105 0.003475 0.002705 0.002462 0.000645 0.001694 0.003059\n",
      "  0.002005]\n",
      " [0.004329 0.000116 0.003556 0.004202 0.002798 0.001259 0.001109 0.004258\n",
      "  0.000947 0.000495 0.107216 0.010038 0.000042 0.005393 0.008556 0.001139\n",
      "  0.001342 0.000543 0.001706 0.001329 0.005406 0.001417 0.003720 0.006718\n",
      "  0.004403]\n",
      " [0.004732 0.000127 0.003888 0.004594 0.003058 0.001376 0.001212 0.004655\n",
      "  0.001035 0.000541 0.007889 0.110974 0.000045 0.005896 0.009354 0.001245\n",
      "  0.001467 0.000593 0.001866 0.001452 0.005910 0.001549 0.004067 0.007344\n",
      "  0.004813]\n",
      " [0.003758 0.000101 0.003087 0.003648 0.002429 0.001092 0.000963 0.003696\n",
      "  0.000822 0.000429 0.006264 0.008714 0.100036 0.004681 0.007427 0.000989\n",
      "  0.001165 0.000471 0.001481 0.001153 0.004693 0.001230 0.003229 0.005831\n",
      "  0.003822]\n",
      " [0.004655 0.000125 0.003824 0.004519 0.003008 0.001353 0.001192 0.004578\n",
      "  0.001018 0.000532 0.007759 0.010794 0.000045 0.105799 0.009200 0.001225\n",
      "  0.001443 0.000584 0.001835 0.001429 0.005813 0.001523 0.004000 0.007223\n",
      "  0.004734]\n",
      " [0.004340 0.000117 0.003566 0.004213 0.002805 0.001262 0.001112 0.004269\n",
      "  0.000949 0.000496 0.007235 0.010065 0.000042 0.005407 0.108579 0.001142\n",
      "  0.001346 0.000544 0.001711 0.001332 0.005421 0.001420 0.003730 0.006735\n",
      "  0.004414]\n",
      " [0.005983 0.000161 0.004915 0.005808 0.003867 0.003862 0.003403 0.013067\n",
      "  0.002906 0.001518 0.003628 0.005047 0.000021 0.002711 0.004302 0.104655\n",
      "  0.005486 0.002218 0.006975 0.005431 0.003419 0.000896 0.002353 0.004248\n",
      "  0.002784]\n",
      " [0.006391 0.000172 0.005250 0.006204 0.004130 0.004126 0.003635 0.013958\n",
      "  0.003104 0.001621 0.003875 0.005391 0.000022 0.002896 0.004595 0.004973\n",
      "  0.105860 0.002370 0.007451 0.005801 0.003652 0.000957 0.002513 0.004538\n",
      "  0.002974]\n",
      " [0.005383 0.000145 0.004423 0.005226 0.003479 0.003476 0.003062 0.011759\n",
      "  0.002615 0.001366 0.003265 0.004541 0.000019 0.002440 0.003871 0.004189\n",
      "  0.004936 0.101996 0.006277 0.004887 0.003077 0.000806 0.002117 0.003823\n",
      "  0.002505]\n",
      " [0.006313 0.000170 0.005186 0.006129 0.004080 0.004076 0.003591 0.013788\n",
      "  0.003066 0.001602 0.003828 0.005325 0.000022 0.002861 0.004539 0.004912\n",
      "  0.005788 0.002341 0.107361 0.005731 0.003608 0.000945 0.002483 0.004483\n",
      "  0.002938]\n",
      " [0.005994 0.000161 0.004924 0.005819 0.003874 0.003870 0.003410 0.013092\n",
      "  0.002911 0.001521 0.003635 0.005057 0.000021 0.002717 0.004310 0.004664\n",
      "  0.005496 0.002223 0.006989 0.105442 0.003426 0.000898 0.002357 0.004256\n",
      "  0.002790]\n",
      " [0.006529 0.000176 0.005363 0.006338 0.004219 0.001988 0.001752 0.006727\n",
      "  0.001496 0.000781 0.006602 0.009185 0.000038 0.004934 0.007828 0.002392\n",
      "  0.002818 0.001140 0.003583 0.002790 0.102837 0.000744 0.001953 0.003526\n",
      "  0.002311]\n",
      " [0.006567 0.000177 0.005395 0.006375 0.004244 0.002000 0.001762 0.006767\n",
      "  0.001505 0.000786 0.006641 0.009238 0.000038 0.004963 0.007874 0.002406\n",
      "  0.002835 0.001146 0.003604 0.002806 0.002854 0.100748 0.001964 0.003546\n",
      "  0.002324]\n",
      " [0.006474 0.000174 0.005318 0.006285 0.004184 0.001972 0.001737 0.006671\n",
      "  0.001483 0.000775 0.006547 0.009108 0.000038 0.004893 0.007763 0.002372\n",
      "  0.002794 0.001130 0.003553 0.002767 0.002814 0.000737 0.101936 0.003496\n",
      "  0.002291]\n",
      " [0.006559 0.000176 0.005389 0.006368 0.004239 0.001998 0.001760 0.006759\n",
      "  0.001503 0.000785 0.006634 0.009228 0.000038 0.004958 0.007865 0.002403\n",
      "  0.002831 0.001145 0.003600 0.002803 0.002851 0.000747 0.001962 0.103542\n",
      "  0.002322]\n",
      " [0.006530 0.000176 0.005364 0.006339 0.004220 0.001989 0.001752 0.006728\n",
      "  0.001496 0.000782 0.006604 0.009186 0.000038 0.004935 0.007830 0.002392\n",
      "  0.002819 0.001140 0.003584 0.002791 0.002838 0.000744 0.001953 0.003526\n",
      "  0.102311]]\n",
      "[0.011601 0.000312 0.009531 0.011263 0.007498 0.005734 0.005052 0.019400\n",
      " 0.004314 0.002253 0.010541 0.014664 0.000061 0.007878 0.012498 0.006678\n",
      " 0.007869 0.003182 0.010006 0.007791 0.008864 0.002323 0.006100 0.011014\n",
      " 0.007219]\n"
     ]
    }
   ],
   "source": [
    "print(P_theta)\n",
    "print((1-gamma)*(np.linalg.inv((np.identity(n*m) - gamma*P_theta))))\n",
    "print(d_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Input: probability vector, state, action\n",
    "Output: \\nabla_{\\theta} \\pi_{\\theta}(s,a)\n",
    "\n",
    "States go from 0 to n-1 and actons from 0 to m-1\n",
    "'''\n",
    "def grad_state_xy(qvals,prob,state_x,state_y):\n",
    "    grad = np.zeros(2*n)\n",
    "    Q_s_0 = qvals[2*(state_x*m + state_y) + 1]\n",
    "    Q_s_1 = qvals[2*(state_x*m + state_y)]\n",
    "    pi_s_1 = prob[2*(state_x*m + state_y) + 1]\n",
    "    grad[2*state_x] = - (Q_s_0 - Q_s_1)*pi_s_1*(1-pi_s_1)*theta[2*state_x + 1]\n",
    "    grad[2*state_x + 1] = (Q_s_0 - Q_s_1)*pi_s_1*(1-pi_s_1)*offer[state_y]\n",
    "    return grad\n",
    "\n",
    "def grad_state(qvals,prob,d_pi,state_x):\n",
    "    grad = np.sum([d_pi[state_x*m + j]*grad_state_xy(qvals,prob,state_x,j) for j in range(m)],axis=0)        \n",
    "    return grad\n",
    "\n",
    "def grad(qvals,prob,d_pi):\n",
    "    grad = np.sum([grad_state(qvals,prob,d_pi,i) for i in range(n)],axis=0)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.046647 -0.004414 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n",
      " 0.000000 0.000000]\n",
      "[-0.001080 0.001042 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n",
      " 0.000000 0.000000]\n",
      "[-0.001080 0.001042 -0.001756 0.001990 0.000249 0.000118 0.000159 0.000288\n",
      " -0.000023 0.000707]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Check: only non-zero components should correspond to the given state\n",
    "'''\n",
    "grad_xy = grad_state_xy(qvals,prob,0,1)\n",
    "print(grad_xy)\n",
    "grad_x = grad_state(qvals,prob,d_pi,0)\n",
    "print(grad_x)\n",
    "grad_final = grad(qvals,prob,d_pi)\n",
    "print(grad_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The overall reward function \\ell(\\theta)\n",
    "'''\n",
    "def ell(qvals,prob,rho):\n",
    "    V = np.zeros(n*m)\n",
    "    for i in range(n*m):\n",
    "        V[i] = np.sum([qvals[2*i + j]*prob[2*i + j] for j in range(2)])\n",
    "    \n",
    "    ell = np.dot(V,rho.flatten())\n",
    "    return ell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average reward', 0.35168830933828854)\n"
     ]
    }
   ],
   "source": [
    "avg_reward = ell(qvals,prob,rho)\n",
    "print('Average reward',avg_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtracking line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ell_theta(theta,rho):\n",
    "    prob = theta_to_policy(theta,n,m)\n",
    "    Pi = get_Pi(prob,n,m)\n",
    "    mat = np.identity(2*n*m) - gamma*np.matmul(prob_trans,Pi)\n",
    "    qvals = np.dot(np.linalg.inv(mat),reward)\n",
    "    return ell(qvals,prob,rho)\n",
    "       \n",
    "def find_step(theta,gradient,alpha,beta):\n",
    "    step = alpha\n",
    "    while ell_theta(theta - step*gradient,rho) > ell_theta(theta,rho) - (step/2)*np.linalg.norm(gradient):\n",
    "        step = beta*step\n",
    "    return step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration to find optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.357182 0.079613]\n",
      " [0.305460 0.330719]\n",
      " [0.773830 0.039959]\n",
      " [0.429492 0.314927]\n",
      " [0.636491 0.346347]\n",
      " [0.043097 0.879915]\n",
      " [0.763241 0.878097]\n",
      " [0.417509 0.605578]\n",
      " [0.513467 0.597837]\n",
      " [0.262216 0.300871]\n",
      " [0.025400 0.303063]\n",
      " [0.242076 0.557578]\n",
      " [0.565507 0.475132]\n",
      " [0.292798 0.064251]\n",
      " [0.978819 0.339708]\n",
      " [0.495049 0.977081]\n",
      " [0.440774 0.318273]\n",
      " [0.519797 0.578136]\n",
      " [0.853934 0.068097]\n",
      " [0.464531 0.781949]\n",
      " [0.718603 0.586022]\n",
      " [0.037094 0.350656]\n",
      " [0.563191 0.299730]\n",
      " [0.512334 0.673467]\n",
      " [0.159194 0.050478]]\n",
      "('prob_vec', array([[0.817733, 0.182267],\n",
      "       [0.480148, 0.519852],\n",
      "       [0.950897, 0.049103],\n",
      "       [0.576949, 0.423051],\n",
      "       [0.647605, 0.352395],\n",
      "       [0.046692, 0.953308],\n",
      "       [0.465011, 0.534989],\n",
      "       [0.408088, 0.591912],\n",
      "       [0.462040, 0.537960],\n",
      "       [0.465675, 0.534325],\n",
      "       [0.077329, 0.922671],\n",
      "       [0.302726, 0.697274],\n",
      "       [0.543423, 0.456577],\n",
      "       [0.820050, 0.179950],\n",
      "       [0.742358, 0.257642],\n",
      "       [0.336281, 0.663719],\n",
      "       [0.580694, 0.419306],\n",
      "       [0.473432, 0.526568],\n",
      "       [0.926144, 0.073856],\n",
      "       [0.372674, 0.627326],\n",
      "       [0.550812, 0.449188],\n",
      "       [0.095666, 0.904334],\n",
      "       [0.652656, 0.347344],\n",
      "       [0.432057, 0.567943],\n",
      "       [0.759253, 0.240747]]))\n",
      "('Initial policy', array([0.817733, 0.182267, 0.480148, 0.519852, 0.950897, 0.049103,\n",
      "       0.576949, 0.423051, 0.647605, 0.352395, 0.046692, 0.953308,\n",
      "       0.465011, 0.534989, 0.408088, 0.591912, 0.462040, 0.537960,\n",
      "       0.465675, 0.534325, 0.077329, 0.922671, 0.302726, 0.697274,\n",
      "       0.543423, 0.456577, 0.820050, 0.179950, 0.742358, 0.257642,\n",
      "       0.336281, 0.663719, 0.580694, 0.419306, 0.473432, 0.526568,\n",
      "       0.926144, 0.073856, 0.372674, 0.627326, 0.550812, 0.449188,\n",
      "       0.095666, 0.904334, 0.652656, 0.347344, 0.432057, 0.567943,\n",
      "       0.759253, 0.240747]))\n"
     ]
    }
   ],
   "source": [
    "raw_vec = np.random.uniform(0,1,size=(n*m,2))\n",
    "print(raw_vec)\n",
    "prob_vec = raw_vec/raw_vec.sum(axis=1,keepdims=1)\n",
    "print('prob_vec',prob_vec)\n",
    "init_policy = prob_vec.flatten()\n",
    "print('Initial policy',init_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Policy iteration function\n",
    "'''\n",
    "def policy_iter(q_vals,n,m):\n",
    "    new_policy = np.zeros(2*n*m)\n",
    "    for i in range(n*m):\n",
    "        idx = np.argmax(q_vals[2*i:2*(i+1)])\n",
    "        new_policy[2*i + idx] = 1\n",
    "    \n",
    "    return new_policy   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.317742 0.392529 0.317742 0.093460 0.317742 0.821106 0.317742 0.151152\n",
      " 0.317742 0.384114 0.312370 0.392529 0.312370 0.093460 0.312370 0.821106\n",
      " 0.312370 0.151152 0.312370 0.384114 0.289641 0.392529 0.289641 0.093460\n",
      " 0.289641 0.821106 0.289641 0.151152 0.289641 0.384114 0.332254 0.392529\n",
      " 0.332254 0.093460 0.332254 0.821106 0.332254 0.151152 0.332254 0.384114\n",
      " 0.303309 0.392529 0.303309 0.093460 0.303309 0.821106 0.303309 0.151152\n",
      " 0.303309 0.384114]\n",
      "('Improved policy', array([0.000000, 1.000000, 1.000000, 0.000000, 0.000000, 1.000000,\n",
      "       1.000000, 0.000000, 0.000000, 1.000000, 0.000000, 1.000000,\n",
      "       1.000000, 0.000000, 0.000000, 1.000000, 1.000000, 0.000000,\n",
      "       0.000000, 1.000000, 0.000000, 1.000000, 1.000000, 0.000000,\n",
      "       0.000000, 1.000000, 1.000000, 0.000000, 0.000000, 1.000000,\n",
      "       0.000000, 1.000000, 1.000000, 0.000000, 0.000000, 1.000000,\n",
      "       1.000000, 0.000000, 0.000000, 1.000000, 0.000000, 1.000000,\n",
      "       1.000000, 0.000000, 0.000000, 1.000000, 1.000000, 0.000000,\n",
      "       0.000000, 1.000000]))\n"
     ]
    }
   ],
   "source": [
    "print(qvals)\n",
    "new_policy = policy_iter(qvals,n,m)\n",
    "print('Improved policy',new_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Starting policy', array([0.817733, 0.182267, 0.480148, 0.519852, 0.950897, 0.049103,\n",
      "       0.576949, 0.423051, 0.647605, 0.352395, 0.046692, 0.953308,\n",
      "       0.465011, 0.534989, 0.408088, 0.591912, 0.462040, 0.537960,\n",
      "       0.465675, 0.534325, 0.077329, 0.922671, 0.302726, 0.697274,\n",
      "       0.543423, 0.456577, 0.820050, 0.179950, 0.742358, 0.257642,\n",
      "       0.336281, 0.663719, 0.580694, 0.419306, 0.473432, 0.526568,\n",
      "       0.926144, 0.073856, 0.372674, 0.627326, 0.550812, 0.449188,\n",
      "       0.095666, 0.904334, 0.652656, 0.347344, 0.432057, 0.567943,\n",
      "       0.759253, 0.240747]))\n",
      "50\n",
      "20\n",
      "0\n",
      "('Final policy', array([1.000000, 0.000000, 1.000000, 0.000000, 0.000000, 1.000000,\n",
      "       1.000000, 0.000000, 1.000000, 0.000000, 1.000000, 0.000000,\n",
      "       1.000000, 0.000000, 0.000000, 1.000000, 1.000000, 0.000000,\n",
      "       1.000000, 0.000000, 1.000000, 0.000000, 1.000000, 0.000000,\n",
      "       0.000000, 1.000000, 1.000000, 0.000000, 1.000000, 0.000000,\n",
      "       1.000000, 0.000000, 1.000000, 0.000000, 0.000000, 1.000000,\n",
      "       1.000000, 0.000000, 1.000000, 0.000000, 1.000000, 0.000000,\n",
      "       1.000000, 0.000000, 0.000000, 1.000000, 1.000000, 0.000000,\n",
      "       1.000000, 0.000000]))\n"
     ]
    }
   ],
   "source": [
    "curr_policy = np.random.uniform(0,1,size=(2*n*m))\n",
    "new_policy = init_policy\n",
    "print('Starting policy',init_policy)\n",
    "\n",
    "while np.count_nonzero(curr_policy - new_policy) > 0:\n",
    "    curr_policy = new_policy\n",
    "    Pi = get_Pi(curr_policy,n,m)\n",
    "    mat = np.identity(2*n*m) - gamma*np.matmul(prob_trans,Pi)\n",
    "    q_vals = np.dot(np.linalg.inv(mat),reward)\n",
    "    new_policy = policy_iter(q_vals,n,m)\n",
    "    print(np.count_nonzero(curr_policy - new_policy))\n",
    "    \n",
    "print('Final policy',new_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5713858790238419\n"
     ]
    }
   ],
   "source": [
    "ell_star = ell(q_vals,new_policy,rho)\n",
    "print(ell_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy gradient in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Optimality gap', 0.22619905422886416)\n",
      "('Optimality gap', 0.18413361069412698)\n",
      "('Optimality gap', 0.1752023629344841)\n",
      "('Optimality gap', 0.17140007401798824)\n",
      "('Optimality gap', 0.16770111989310194)\n",
      "('Optimality gap', 0.16354495726721352)\n",
      "('Optimality gap', 0.159296330441755)\n",
      "('Optimality gap', 0.1553848635410543)\n",
      "('Optimality gap', 0.15191665842273144)\n",
      "('Optimality gap', 0.1489006642400087)\n",
      "('Optimality gap', 0.14634189889870486)\n",
      "('Optimality gap', 0.14422194517358394)\n",
      "('Optimality gap', 0.14245108463653633)\n",
      "('Optimality gap', 0.14077402707407166)\n",
      "('Optimality gap', 0.13876056709010398)\n",
      "('Optimality gap', 0.13657186184335357)\n",
      "('Optimality gap', 0.13433512410133386)\n",
      "('Optimality gap', 0.13212987942549526)\n",
      "('Optimality gap', 0.12999863959999824)\n",
      "('Optimality gap', 0.12790037732401643)\n",
      "('Optimality gap', 0.12572591640964065)\n",
      "('Optimality gap', 0.1232749006369126)\n",
      "('Optimality gap', 0.12014199673987963)\n",
      "('Optimality gap', 0.11532203785143358)\n",
      "('Optimality gap', 0.10581982121357397)\n",
      "('Optimality gap', 0.08679773332547475)\n",
      "('Optimality gap', 0.07104437154111953)\n",
      "('Optimality gap', 0.06395693528422719)\n",
      "('Optimality gap', 0.05985722606441335)\n",
      "('Optimality gap', 0.056875069789843025)\n",
      "('Optimality gap', 0.05446338390958383)\n",
      "('Optimality gap', 0.052411705867309255)\n",
      "('Optimality gap', 0.050617697314861765)\n",
      "('Optimality gap', 0.04902233702774805)\n",
      "('Optimality gap', 0.047587273708629474)\n",
      "('Optimality gap', 0.046285455693613975)\n",
      "('Optimality gap', 0.045096689411028956)\n",
      "('Optimality gap', 0.0440052880377948)\n",
      "('Optimality gap', 0.0429987094850629)\n",
      "('Optimality gap', 0.042066707816458604)\n",
      "('Optimality gap', 0.04120077265325339)\n",
      "('Optimality gap', 0.040393741052462295)\n",
      "('Optimality gap', 0.03963951851897085)\n",
      "('Optimality gap', 0.03893287232048159)\n",
      "('Optimality gap', 0.038269274572042655)\n",
      "('Optimality gap', 0.03764478068758459)\n",
      "('Optimality gap', 0.037055933640289807)\n",
      "('Optimality gap', 0.0364996874811756)\n",
      "('Optimality gap', 0.035973345501140175)\n",
      "('Optimality gap', 0.035474509707853485)\n",
      "('Optimality gap', 0.03500103916732478)\n",
      "('Optimality gap', 0.03455101537466165)\n",
      "('Optimality gap', 0.03412271325785332)\n",
      "('Optimality gap', 0.03371457673827438)\n",
      "('Optimality gap', 0.033325198008330226)\n",
      "('Optimality gap', 0.032953299864407426)\n",
      "('Optimality gap', 0.03259772056846921)\n",
      "('Optimality gap', 0.03225740081563233)\n",
      "('Optimality gap', 0.03193137246588773)\n",
      "('Optimality gap', 0.03161874876155679)\n",
      "('Optimality gap', 0.031318715802233776)\n",
      "('Optimality gap', 0.031030525088990735)\n",
      "('Optimality gap', 0.030753486981744405)\n",
      "('Optimality gap', 0.030486964939661854)\n",
      "('Optimality gap', 0.030230370435608056)\n",
      "('Optimality gap', 0.029983158452913217)\n",
      "('Optimality gap', 0.029744823486957173)\n",
      "('Optimality gap', 0.02951489598579915)\n",
      "('Optimality gap', 0.02929293917384368)\n",
      "('Optimality gap', 0.029078546210652534)\n",
      "('Optimality gap', 0.028871337643834316)\n",
      "('Optimality gap', 0.028670959120660533)\n",
      "('Optimality gap', 0.028477079327893784)\n",
      "('Optimality gap', 0.028289388133405313)\n",
      "('Optimality gap', 0.028107594906638278)\n",
      "('Optimality gap', 0.027931426997939046)\n",
      "('Optimality gap', 0.027760628359317252)\n",
      "('Optimality gap', 0.027594958291369376)\n",
      "('Optimality gap', 0.02743419030297589)\n",
      "('Optimality gap', 0.027278111071997846)\n",
      "('Optimality gap', 0.02712651949659195)\n",
      "('Optimality gap', 0.026979225827982845)\n",
      "('Optimality gap', 0.026836050876576922)\n",
      "('Optimality gap', 0.026696825284226122)\n",
      "('Optimality gap', 0.026561388856248924)\n",
      "('Optimality gap', 0.026429589947517984)\n",
      "('Optimality gap', 0.026301284897542687)\n",
      "('Optimality gap', 0.026176337510009362)\n",
      "('Optimality gap', 0.02605461857272595)\n",
      "('Optimality gap', 0.02593600541433483)\n",
      "('Optimality gap', 0.025820381494527145)\n",
      "('Optimality gap', 0.02570763602482573)\n",
      "('Optimality gap', 0.02559766361729543)\n",
      "('Optimality gap', 0.025490363958792228)\n",
      "('Optimality gap', 0.025385641508604118)\n",
      "('Optimality gap', 0.02528340521753647)\n",
      "('Optimality gap', 0.025183568266676448)\n",
      "('Optimality gap', 0.025086047824242952)\n",
      "('Optimality gap', 0.024990764819068922)\n",
      "('Optimality gap', 0.02489764372940051)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Gradient decent\n",
    "'''\n",
    "N = 100000\n",
    "stepsize = 0.01\n",
    "# Parameters for line search\n",
    "alpha = 1\n",
    "beta = 0.7\n",
    "theta = np.random.uniform(0,1,size=2*n)\n",
    "gap = []\n",
    "for k in range(N):\n",
    "    prob = theta_to_policy(theta,n,m)\n",
    "\n",
    "    Pi = get_Pi(prob,n,m)\n",
    "    mat = np.identity(2*n*m) - gamma*np.matmul(prob_trans,Pi)\n",
    "    qvals = np.dot(np.linalg.inv(mat),reward)\n",
    "\n",
    "    P_theta = np.matmul(Pi,prob_trans)\n",
    "    d_pi = (1-gamma)*np.dot(np.transpose((np.linalg.inv(np.identity(n*m) - gamma*P_theta))),rho.flatten())\n",
    "\n",
    "    gradient = grad(qvals,prob,d_pi)\n",
    "    #     theta += stepsize*gradient\n",
    "    \n",
    "    step = find_step(theta,gradient,alpha,beta)\n",
    "    theta += step*gradient\n",
    "        \n",
    "    if k % 1000 == 0:\n",
    "        avg_reward = ell(qvals,prob,rho)\n",
    "        print('Optimality gap',ell_star - avg_reward)\n",
    "        gap.append(ell_star - avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x111680890>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1116850d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure\n",
    "plt.plot(np.array(gap))\n",
    "plt.title('Optimality gap during training')\n",
    "plt.ylabel('Optimality gap')\n",
    "plt.xlabel('Iteration number/100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
